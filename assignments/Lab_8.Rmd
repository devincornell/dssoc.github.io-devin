---
title: "Lab #8: Basics of Text Analysis"
author: "Data Science and Society (Sociology 367)"
date: ""
output: html_document
---

In this lab, we will practice making statistical models of data using R. Throughout the lab we'll be using these models to understand how Spotify calculates their "popularity" scores assigned to each artist. Edit the Lab #7 markdown file ([link to file](https://github.com/dssoc/dssoc.github.io/raw/master/assignments/Lab_7.Rmd)) for your submission. Remember to change the "author" field above to your name. **Send the markdown file (.Rmd) to your TA via direct message on Slack.** Be sure to do the required reading!

**Required reading:** 

* R for Data Science: [Modeling (Chapter 14)](https://r4ds.had.co.nz/strings.html)


**Optional reading:** 

* [stringr package docs](https://www.rdocumentation.org/packages/stringr/versions/1.4.0)
* Test and develop regex expressions on [regexr.com](https://regexr.com/)



### Required Datasets

We will use the following datasets for the exercises.

(1) `artist_meta`: [artist_meta.Rdata](https://github.com/dssoc/dssoc.github.io/raw/master/datasets/artist_meta.Rdata) is a table of Grammy nominee names and some of their spotify metadata.


**Load the datasets and libraries. NOTE! It might be a good idea to convert the `popularity` column to a "double" type here to make later analyses easier. Do that here using the `mutate` and `as.integer` functions.**
```{r message=FALSE}
library(tidyverse)
library(stringr)
library(tidytext)
library(tm)

# THIS SHOULD WORK AS-IS
load(url('https://dssoc.github.io/datasets/Coronavirus_Tweets.Rdata'))
```
<br/>



## Exercises
<br/>

**1. Create a regular expression which matches a URL in the example string `ex`, and verify that it works using `str_view_all` (described in R for data science [Ch. 14](https://r4ds.had.co.nz/strings.html)). The output should show both URLs highlighted. Now do the same for hashtags. **
Hint: these are common tasks in cleaning and analyzing Tweet/text data, so doing some research might save you a lot of time.
```{r}
# Your answer here

ex <- "BREAKING NEWS - @brumleader urges everyone to do their bit in order to tackle the threat posed by rising coronavirus case numbers in city. Full statement here:\n\nhttps://t.co/3tbc6xcRFP\n\n#KeepBrumSafe\n#Btogether\n#COVID19\n#Coronavirus https://t.co/mo5bPUgGgC"

url_re <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
ht_re <- "#\\w*[a-zA-Z]\\w*"

str_view_all(ex, url_re)
str_view_all(ex, ht_re)
```
**2. Add two new columns to the `covid_tweets` dataframe: `n_link` should include the number of URLs in the Tweet text, and `n_ht` should be the number of hashtags. Then, create a linear model predicting `retweet_count` from `n_link` and `n_ht`.  Were either of these predictors statistically significant? Are they significant predictors of `favorite_count`? Be sure to show the model summaries.**
Hint: be sure to read the stringr documentation.
```{r}
covid_tweets <- covid_tweets %>% 
  mutate(n_link=(str_count(tweet_text, url_re))) %>% 
  mutate(n_ht=(str_count(tweet_text, ht_re)))

m1 <- lm(retweet_count ~ n_link + n_ht, data=covid_tweets)
m2 <- lm(favorite_count ~ n_link + n_ht, data=covid_tweets)
summary(m1)
summary(m2)
```



**3. Using stringr and dplyr, produce a dataframe consisting of the 5 most used hashtags in our Tweets with the number of times they were used.**
Hints: (1) you may want to check out the `unnest` function; (2) for reference, there are 55 unique hashtags in the dataset when ignoring capitalization.
```{r}
covid_tweets %>% 
  mutate(ht=str_extract_all(tweet_text, ht_re)) %>% 
  unnest(ht) %>%
  mutate(ht=str_to_lower(ht)) %>% 
  group_by(ht) %>% 
  count() %>% 
  arrange(desc(n))
```


**4. Create a new column in `covid_tweets` called `cleaned` which includes the original Tweets with hashtags and links removed. We will use this column for the remaining questions.** 
```{r}
covid_tweets <- covid_tweets %>% 
  mutate(cleaned=gsub(url_re, "", tweet_text)) %>% 
  mutate(cleaned=gsub(ht_re, "", cleaned))
covid_tweets %>% select(cleaned)
```



**5. Using tidytext, produce a dataframe showing the ten most common words in English-language Tweets after URLs and hashtags have been removed (use our new column `cleaned`). Then secondly show the most common words excluding stopwords.**
Hint: look at the tidytext docs for `unnest_tokens`.
```{r}
topwords <- covid_tweets %>% 
  filter(language=='en') %>% 
  unnest_tokens('word', cleaned) %>% 
  group_by(word) %>% count() %>% 
  arrange(desc(n))

topwords
topwords %>% anti_join(stop_words)


```
<br/>


**5. Create a document-term matrix which including english-language Tweets. We will discuss what to do with a dtm in the next lab.**
```{r}
dtm <- covid_tweets %>% 
  filter(language=='en') %>% 
  unnest_tokens('word', cleaned) %>% 
  count(date, word) %>% 
  cast_dtm(date, word, n)
dtm
```


**6. How could you potentially use text analysis in the final project you have been working on? (You don't necessarily need to do it for the project, just think hypothetically).**
```
response
```

**7. Last week you proposed some datasets that you might be able to use for our final projects in the class. If you haven't yet, try to download or otherwise get access to the dataset so you can start playing with it. Either way, what did you find? Did your data have the information you needed after all? Was it as easy to access as you expected? Even if you're not able to get all the data by now, write something about your plan for getting access to the data.**
```
response
```




