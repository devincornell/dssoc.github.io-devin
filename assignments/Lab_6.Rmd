---
title: "Lab #6: Working with APIs"
author: "Data Science and Society (Sociology 367)"
date: ""
output: html_document
---


In this lab, we will practice working with application programming interfaces (APIs). Edit the Lab #6 markdown file ([link to file](https://github.com/dssoc/dssoc.github.io/raw/master/assignments/Lab_6.Rmd)) for your submission. Remember to change the "author" field above to your name. **Send the markdown file (.Rmd) to your TA via direct message on Slack.** Be sure to do the required reading!

**Required reading:** 

* [Intro to APIs](https://medium.com/@rwilliams_bv/apis-d389aa68104f), by Beck Williams
* [An Illustrated Introduction to APIs](https://medium.com/epfl-extension-school/an-illustrated-introduction-to-apis-10f8000313b9), by Xavier Adam
* [Obtaining and using access tokens for Twitter](https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html)

**Optional reading:** [Intro to Network Analysis with R, by Jesse Sadle](https://www.jessesadler.com/post/network-analysis-with-r/).



### Required Datasets

We will use the following datasets for the exercises.

(1) `senator_data`: [Senator_Profiles.Rdata](https://github.com/dssoc/dssoc.github.io/raw/master/datasets/Senator_Profiles.Rdata) includes information about all senators, including their twitter handles (in the `screen_name` column).


**Load the datasets and libraries:**
```{r}
library(tidyverse)
library(rtweet)
load('../datasets/Senator_Profiles.Rdata')
```
<br/><br/>


### API Setup

Follow these two steps to set up your program for exercise.

#### 1. Set up your API credentials with Twitter. 
If you don't already have one, you will need to create a new account. Instructions for extracting the api key can be found [here](https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html).

#### 2. Store your credentials.
Copy and paste the json below into a new json file named `api_credentials.json`. From the instructions in Step 1 we have the api key, api secret key, access token, and access token secrets - replace the approprate values in the json file and save.

```
{
  "app": "<app name here>",
	"api_key": "<api key here>",
	"api_secret_key": "<api_secret_key here",
	"access_token": "<access token here>",
	"access_token_secret": "<access token secret here>",
	"bearer_token": "<unused>"
}
```

#### 3. Authenticate your application.

After you have the credentials stored into the json file, run this code to authenticate the application. This simply reads the json data and provides them directly to the `create_token` function of the `rtweet` package. Once you complete this step, you should be able to access Twitter data through the API. See the `rtweet` package documentation to see how to access different types of data.

```{r, eval = F}

# this code will read credentials from the JSON file you created.
#install.packages("rjson")
library('rjson')
creds <- fromJSON(file = 'api_credentials.json')

# will allow you to authenticate your application
token <- create_token(
  app = creds$app,
  consumer_key = creds$api_key,
  consumer_secret = creds$api_secret_key,
  access_token = creds$access_token,
  access_secret = creds$access_token_secret)

# this allows you to check the remaining data
lim <- rate_limit()
lim[, 1:4]

```



## Exercises
<br/>

**1. In your own words, describe what an application programming interface is, and what advantage it provides to data scientists/computational social scientists.**

```{r}
# Your answer here
```
<br/>


**2. Use the API to augment `senator_data` with additional data from the Twitter platform. To do this, you will need to request information about each senator using the API, and then join (see `left_join`) the Twitter data into the existing dataframe to make a new dataframe called `senator_accounts`.**
```{r}
# Your answer here
users <- lookup_users(senator_data$screen_name)
senator_accounts <- senator_data %>% left_join(users, by='user_id')
```
<br/>

**3. The dataframe we created above will contain information about the Senators' posted status (a pinned Tweet or the last in their timeline), and the users that they mention in those Tweets (see the column `mentions_screen_name`). Calculate the average probability of a Senator mentioning another user in their status by political party affiliation. For instance, you should be able to say "X% of Republicans and Y% of Democrats mention another user in their statuses.**
```{r}
# Your answer here
senator_twitters <- senator
```
<br/>

**4. What conclusions can you draw from the previous result? What assumptions are needed to come to these conclusions?**

```
# Your answer here
senator_twitters <- senator
```

**5. Now write a function that takes an arbitrary sequence of Twitter user ids as an argument and returns a dataframe containing their last 10 Tweets, then use this function to make a dataframe consisting of the last 10 tweets from all senators. This should be a dataframe with 990 rows, ten for each senator in our dataset. Store this result in a new dataframe called `tweets` - we will use it for later questions in this lab. Hint: for the function, one approach might be to use a loop to get tweets from one user at a time.**
```{r}
# Your answer here
timelines_get <- function(user_ids) {
  all_statuses <- data.frame()
  for (uid in user_ids) {
    statuses <- get_timeline(uid, n=10)
    all_statuses <- bind_rows(all_statuses, statuses)
  }
  return(all_statuses)
}

tweets <- timelines_get(senator_twitter$user_id)
```
<br/>


**6. Now add information from the `senator_accounts` dataframe into the `tweets` dataframe using a join operation (see `left_join`) based on `user_id`. The resulting Tweet dataframe should still have 990 rows, but include additional columns containing information about the senator that created the Tweet. Use this merged dataset and `ggplot` to create a [box plot](https://www.r-graph-gallery.com/boxplot.html) or [violin plot](https://www.r-graph-gallery.com/violin.html) showing the average number of **
```{r}
# Your answer here

tweets %>%
  ggplot(aes(x=name, y=value, fill=name)) +
    geom_violin() +
    #scale_fill_viridis(discrete = TRUE, alpha=0.6, option="A") +
    #theme_ipsum() +
    #theme(
    #  legend.position="none",
    #  plot.title = element_text(size=11)
    #) +
    ggtitle("Violin chart") +
    xlab("")
```
<br/>


**4. We will now generate some statistics about the tweet data we have extracted from the API. First, compute **
```{r}
# Your answer here

tweets %>%
  ggplot(aes(x=name, y=value, fill=name)) +
    geom_violin() +
    #scale_fill_viridis(discrete = TRUE, alpha=0.6, option="A") +
    #theme_ipsum() +
    #theme(
    #  legend.position="none",
    #  plot.title = element_text(size=11)
    #) +
    ggtitle("Violin chart") +
    xlab("")
```
<br/>







**6. Now, make a network visualization of the edge list created in Exercise 5. You may do this however you'd like. Although not necessary, you may consider reformatting your data so that the network visual contains only nodes for the Grammy nominees (i.e. those Spotify ids in the `artist_meta` dataset), with edges weighted by the number of related artists they share in common. You may also consider adding artist names from the `artist_meta` dataset as node attributes in your visualization. Alternatively, you may like to gather more information from the Spotify API and add it as node or edge information in your visualization (e.g. coloring nodes by primary genre that the artist belongs to). Be creative :)**
```{r}
# Your answer here
```
<br/>


**7. Again using the `spotifyR` package, decide for yourself on some interesting data that can be added to one of the existing Grammy datasets. What new insight (e.g. a summary statistic or pattern) can you gain from this additional data? What more does it tell you about the nominees, the winners, the songs, or how any/all of these entities relate to one another?**
```{r}
# Your answer here
```
<br/>

**8. Identify another API, whether it has an associated R package or not, and describe how you might use the data available from it in a social/data scientific research project.**
```{r}
# Your answer here
```

