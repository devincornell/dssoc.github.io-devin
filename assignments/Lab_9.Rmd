---
title: "Lab #9: Dictionary-Based Text Analysis"
author: "Data Science and Society (Sociology 367)"
date: ""
output: html_document
---

In this lab, we will practice working with text using stringr, tidytext, and tm packages. Edit the Lab #9 markdown file ([link to file](/assignments/Lab_9.Rmd)) for your submission. Remember to change the "author" field above to your name. **Send the markdown file (.Rmd) to your TA via direct message on Slack.** Be sure to do the required reading!

**Required reading:** 

* R for Data Science: [Working with strings (Chapter 14)](https://r4ds.had.co.nz/strings.html)


**Optional reading:** 

* [stringr package docs](https://www.rdocumentation.org/packages/stringr/versions/1.4.0)
* [tidytext package docs](https://www.rdocumentation.org/packages/tidytext/versions/0.2.6)
* [tm package docs](https://cran.r-project.org/web/packages/tm/tm.pdf)
* Test and develop regex expressions on [regexr.com](https://regexr.com/)



### Required Datasets

We will use the following datasets for the exercises.

(1) `covid_tweets`: (load using link below) is a semi-random set of tweets that used the hashtag #coronavirus.


**Load the datasets and libraries. You shouldn't need to change the URL in the `load` function**
```{r message=FALSE}
library(tidyverse)
library(stringr)
library(tidytext)
library(tm)

# THIS SHOULD WORK AS-IS
load(url('https://dssoc.github.io/datasets/Coronavirus_Tweets.Rdata'))
```
<br/>



## Exercises
<br/>


**1. In which scenarios would it be best to consider dictionary-based approaches to text analysis? How does the decision to use dictionary-based approaches shape the research questions you can ask?**
```
# your answer here
```


**2. Create a bar graph showing the frequencies of the twenty most-used tokens in our corpus after removing URLs and stopwords, but preserving hashtags as tokens (e.g. "#covid19" should be a single token). Now create a similar plot that ONLY includes the hashtags.**

Hint: you can do hashtag preservation in many ways, but you might find an easy solution by browsing the documentation for [`unnest_tokens`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.6/topics/unnest_tokens
). Searching on the internet may also be a good strategy.
```{r}
# your answer here
```

**3. Create a bar graph showing the tf-idf scores of the ten tokens with the highest values in our corpus, again preserving hashtags as tokens. What do these scores mean? Give a hypothesis for why these three have the highest values.**
```{r}
# your answer here
```
```
your written explanation here
```

**4. For each of the top-three tf-idf tokens, extract five tweets with the highest number of retweets that include the token. Based on the context provided in these Tweets, give a quick sentence about what they mean. Do they fit your hypotheses from the question before?**
```{r}
# your answer here
```
```
your written explanation here
```


**5. Create a new column in covid_tweets that corresponds to the time of day that a given tweet was posted, and make a bar graph comparing the number of tweets published in day (5am-5pm) vs night.**
```{r}
# your answer here
covid_tweets <- covid_tweets %>% 
  mutate(hr=as.POSIXlt(date)$hour)
hist(covid_tweets$hr, breaks=seq(0, 24, by=3))
```


**6. Use the "bing" sentiment dictionary to compare the average sentiment for Tweets published in daytime vs nighttime using a bar plot. You get to choose how you will create these sentiment scores for comparison. Then, explain and justify your decision.**
```{r}
# your answer here
sent <- get_sentiments("bing")

covid_tweets %>% 
  select(date, tweet_text, hr) %>%
  mutate(daytime=hr>8 & hr<20) %>% 
  unnest_tokens('word', tweet_text) %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(date, daytime, sentiment) #%>% 
  #group_by(daytime, sentiment) %>% mean(n)


```
```
Explain why you chose to compute sentiment in this way.
```
