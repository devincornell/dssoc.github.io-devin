---
title: "Lab #10: Your First Topic Model"
author: "Data Science and Society (Sociology 367)"
date: ""
output: html_document
---

In this lab, we will practice working with topic modeling algorithms. We will use a dataset of wikipedia pages for each senator, and explore the ways topic modeling can help us learn about the corpus. First we will use the LDA algorithm to practice the basics of topic modeling, then we will use the structural topic modeling algorithm (see the `stm` package) to show how we can use information about each senator (age, gender, political party) in conjunction with our model. **NOTE: if you run into problems where your code takes too long to run or your computer freezes up, use the `substr` function to truncate the wikipedia page texts right after loading.**

Edit the Lab #10 markdown file ([link to file](/assignments/Lab_10.Rmd)) for your submission. Remember to change the "author" field above to your name. **Send the markdown file (.Rmd) to your TA via direct message on Slack.** Be sure to do the required reading!


**Required reading:** 

* Text Mining with R: A Tidy Approach, [chapter 6: Topic Modeling by Julia Silge and David Robinson](https://www.tidytextmining.com/topicmodeling.html)
* [IMPORTANT stm Package Vignette](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf)



**Optional reading:** 

* [An intro to topic models by Patrick van Kessel](https://medium.com/pew-research-center-decoded/an-intro-to-topic-models-for-text-analysis-de5aa3e72bdb)

* [topicmodels package docs](https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf)
* [tm package docs](https://cran.r-project.org/web/packages/tm/tm.pdf)
* [stringr package docs](https://www.rdocumentation.org/packages/stringr/versions/1.4.0)
* [tidytext package docs](https://www.rdocumentation.org/packages/tidytext/versions/0.2.6)
* [stm website](https://www.structuraltopicmodel.com/)

### Required Datasets

We will use the following datasets for the exercises. Check the load function to see the URL - if you have problems using load, feel free to download the file automatically.

(1) `congress_wiki`: text of wikipedia articles associated with each member of congress. Not all members of congress have wikipedia pages here.
(2) `congress`: information about each member of congress.


**Load the datasets and libraries. You shouldn't need to change the URL in the `load` function**
```{r message=FALSE}
library(tidytext)
library(tidyverse)
library(tidyr)
library(dplyr)
library(tm)
library(stringr)
library(topicmodels)
library(ggplot2)
library(stm)

# THIS SHOULD WORK AS-IS. If not, download the file from your browswer at the same url.
load(url('https://dssoc.github.io/datasets/congress.RData'))
```
<br/>



## Exercises
<br/>


**1. Describe a document-term matrix (DTM) in your own words. Why is this data structure useful for text analysis?**
```
your answer here
```


**2. Describe a topic modeling algorithm in your own words. What is the input to a topic modeling algorithm (after parsing the raw text)? What does the actual topic model look like? How do you choose the number of topics? What are the beta parameter estimates?**
```
your answer here
```

**3. Join the columns of `congress` with `congress_wiki` so that you should have congress member information (gender, type, etc) associated with every wikipedia page in our dataset, then create a document-term matrix after removing stopwords.**

Hint: your merged dataframe should have as many rows as the original `congress_wiki` dataframe.
```{r}
# your answer here
```

**4. Construct a topic model with LDA using a specified random seed (see the `control` parameter). This might take a little while to run. You can choose the number of topics however you see fit - it might be useful to try multiple values.**
```{r}
# your answer here
```


**5. Make a function that accepts (takes as an argument) a topic model and returns a plot showing the word distributions for the top ten words in each topic, then call the function with your topic model. Choose two topics which appear to be easiest to understand, and explain what you think they represent based on the word distributions.**
```{r}
# your answer here

```
```
your answer here
```

**6. Create a structural topic model with the `stm` package using politician gender, political orientation, type (senate/house), and (approximate) age as covariates in the model. Then use `labelTopics` to view the words associated with two topics you find most interesting. Can you easily describe what the topics are capturing?**

Hint: you'll need to recall some of the string techniques we've used before to calculate age based on birth date. You can use an approximation for age by subtracting the birth year from the current year (2020).
```{r}
# your answer here
```
```
your answer here
```

**7. Now try using `searchK` to identify the best number of topics for your dataset. Which K is optimal? How did you decide?**

Hint: you should be able to use the same preprocessed and prepared objects in from the previous question.
```{r}
# your answer here
```
```
your answer here
```

**8. Use `selectModel` to generate a number of models and then `plotModels` to visualize their performance. Which model had the best performance, and how can you tell? What do the two dimensions plotted using `plotModels` mean? Then use `plot` to show an overview of the topics in the model. What does this tell you? Keep the optimal model for future questions.**

```{r}
# your answer here
```
```
your answer here
```



**9. After identifying the statistically optimal number of topics and choosing the best among models with a given number of topics, we now have a model that we can examine more closely. The first thing we will do is try to understand how each of our covariates (politician age, gender, political party, and type) corresponds to each topic. This is done primarily through use of the `estimateEffect` function. Use `estimateEffect` and `summary` to print out models corresponding to each of our topics. Identify several situations where a covariate is predictive of a topic. Then, create a plot showing those effect sizes with confidence intervals using the `plot` function. Make sure the figure is readable. Which topics are the most interesting based on the covariate significance? What do these results tell you?**
```{r}
# your answer here
```
```
your answer here
```



**10. Now we will inspect the topics in our selected model. First, use `labelTopics` to examine the top words for each topic you identified as interesting in the previous step. Then use `plotQuote` to show the first `n` characters of each document closely associated with the topic. Do you see any interesting patterns?**
```{r}
# your answer here
```
```
your answer here
```


**11. Examine the statistical models and the content of the topics closely to understand the patterns that are appearing in our data. What is your interpretation of the results. You should be able to draw concrete conclusions such as "republican wikipedia pages are more likely to discuss topic X, which is about ..". Based on the nature of the underlying data, why do you think we observe these results?**
```
your answer here
```

**Congratulations! This is the last lab for the course. These labs were not easy, but you persisted and I hope you learned a lot in the process. As you probably noticed by now, learning data science is often about trying a bunch of things and doing research on the web to see what others have done. Of course, it also requires a bit of creativity that comes from experience and intuition about your dataset. Be sure you're talking to professor Bail and the TA to make sure you're on the right track for the final project. Good luck!**

